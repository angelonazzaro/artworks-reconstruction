{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27eca293e0492052",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "5123b96350f60dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:44:17.491643Z",
     "start_time": "2024-05-19T18:44:16.685866Z"
    }
   },
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pygments.formatters import img\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing.fourier_transform import *\n",
    "from clustering.clustering import *\n",
    "from preprocessing.contrast_enhancement import *"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c4cbd2812370b24e",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "To reduce noise in images of whole artworks and fragments, we initially considered using the Fourier transform to process the images in the frequency domain.\n",
    "\n",
    "While converting an image from RGBA to grayscale simplifies processing, it results in the loss of RGB color and alpha channel data, which can be problematic if that information is needed later. Therefore, we chose to split the image into its primary color channels (excluding the alpha channel) and process each channel separately in the frequency domain. After filtering, we planned to reconstruct the filtered image by recombining the processed channels.\n",
    "\n",
    "However, after several trials, we found that processing the channels separately led to significant information loss in one or more channels. Consequently, we decided to use the NLMeansDenoising filter instead.\n",
    "\n",
    "Since our goal is to cluster fragments that belong to the same image, we focus on maintaining \"continuity\" along the fragment borders. Therefore, our process emphasizes the information present along these edges.\n",
    "\n",
    "Steps:\n",
    "1. Extract a working region from the borders of the fragment.\n",
    "2. Filter out the transparent pixels from the working region.\n",
    "3. Denoise the working region.\n",
    "\n",
    "**CONSIDERATION**: Contrast enhancement."
   ]
  },
  {
   "cell_type": "code",
   "id": "7e881ed433270655",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:48:43.534606Z",
     "start_time": "2024-05-19T18:48:40.226241Z"
    }
   },
   "source": [
    "input_dir = \"./data\"\n",
    "files = os.listdir(input_dir)\n",
    "ext = \".png\"\n",
    "threshold = 5\n",
    "path_image_ref = \"./references/5.33.png\"\n",
    "\n",
    "images = []\n",
    "for filename in tqdm(files, total=len(files), desc=\"Pre-processing files: extracting working region, denoising...\"): \n",
    "    if not filename.endswith(ext):\n",
    "        continue \n",
    "    \n",
    "    image = cv.imread(os.path.join(input_dir, filename), cv.IMREAD_UNCHANGED)\n",
    "    working_region = extract_working_region(input_img=image, threshold=threshold)\n",
    "    b, g, r, a = cv.split(working_region)\n",
    "    working_region = filter_working_region(working_region)\n",
    "    # denoise working region \n",
    "    denoised_working_region = cv.fastNlMeansDenoisingColored(working_region)\n",
    "    images.append(denoised_working_region)"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "da623c2ef7db56ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-19T18:48:46.160980Z",
     "start_time": "2024-05-19T18:48:45.715307Z"
    }
   },
   "source": [
    "image_ref = cv.imread(path_image_ref, cv.IMREAD_UNCHANGED)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv.cvtColor(image_ref, cv.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "\n",
    "image_ref = cv.fastNlMeansDenoisingColored(image_ref)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(cv.cvtColor(image_ref, cv.COLOR_BGR2RGB))\n",
    "plt.title(\"Noise Reduction\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ],
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b621d1d1f73e963",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "To extract relevant features from the fragments, we employ two methods:\n",
    "- Color Histograms\n",
    "- Gradient Jacobians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64f6711ab6c7ac0",
   "metadata": {},
   "source": [
    "## Color Histograms\n",
    "\n",
    "Color histograms are graphical representations of the distribution of colors in an image. They quantify the number of pixels that have specific color values, effectively capturing the color composition of the image. By analyzing the color histograms of image fragments, we can compare and cluster similar fragments based on their color distributions.\n",
    "\n",
    "**This technique is particularly useful for identifying and matching regions of images that share similar color patterns**."
   ]
  },
  {
   "cell_type": "code",
   "id": "e68fc1ff8d4286be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:49:00.690801Z",
     "start_time": "2024-05-19T18:49:00.680245Z"
    }
   },
   "source": [
    "unflatten_color_histograms_fragments, unflatten_color_histogram_image_ref = compute_color_histograms(images, image_ref, flatten=False)\n",
    "unflatten_color_histogram_image_ref"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "dda2d0459364a6c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:49:25.276849Z",
     "start_time": "2024-05-19T18:49:25.027870Z"
    }
   },
   "source": [
    "distance_matrix_color_histogram = compute_color_histogram_dist_matrix(unflatten_color_histograms_fragments)\n",
    "distance_matrix_color_histogram"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8ac9ce26-022d-4d08-bca3-cec365f503bc",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db0b5ba-22b3-4536-8bb2-b52a116a914e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:47:09.467379Z",
     "start_time": "2024-05-17T20:47:09.060726Z"
    }
   },
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "fit_kmeans = kmeans.fit(flatten_color_histograms)\n",
    "create_cluster_dirs(data_dir=\"./data\", output_dir=\"clusters/kmeans/colors\", labels=fit_kmeans.labels_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "89d721fc-2ad9-4061-8fa5-842d1b7a22cd",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d471824f-22e2-4563-bbff-3c52bbd88334",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:47:09.563025Z",
     "start_time": "2024-05-17T20:47:09.473107Z"
    }
   },
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.2, min_samples=10, metric=\"precomputed\")\n",
    "fit_db = db.fit(distance_matrix_color_histogram)\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(fit_db.labels_)) - (1 if -1 in fit_db.labels_ else 0)\n",
    "n_noise_ = list(fit_db.labels_).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "create_cluster_dirs(data_dir=\"./data\", output_dir=\"clusters/dbscan/colors\", labels= fit_db.labels_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "784aba06-659c-4e24-9369-09e343906664",
   "metadata": {},
   "source": [
    "## Gradient Jacobians\n",
    "\n",
    "Gradient Jacobians represent the gradients of pixel intensities in an image. They capture the rate of change of pixel values in both the horizontal and vertical directions, highlighting edges and texture details. By computing the Jacobians of image fragments, we can compare and group fragments that exhibit similar edge and texture patterns. Formally, the gradient jacobians we use are of the form:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{bmatrix} G_x & G_{x\\_gray} \\\\ G_y & G_{y\\_gray} \\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $G_x$ and $G_y$ are the aggregated gradient of the RGB channels, while $G_{x\\_gray}$ and $G_{y\\_gray}$ are the gradient of the grayscale image.\n",
    "\n",
    "This method is especially valuable for identifying structural similarities and continuities between different fragments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278a0609-1f41-4eae-8420-bb684f9d12e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:47:09.689047Z",
     "start_time": "2024-05-17T20:47:09.563755Z"
    }
   },
   "source": [
    "unflatten_jacobians_fragments, unflatten_jacobian_image_ref = compute_jacobians(images, image_ref, flatten=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60522d7-bb8d-4968-8c70-2eda1c4b48b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:49:12.224509Z",
     "start_time": "2024-05-17T20:48:47.297184Z"
    }
   },
   "source": [
    "distance_matrix_jacobians = compute_jacobians_dist_matrix(unflatten_jacobians_fragments, unflatten_jacobian_image_ref)\n",
    "distance_matrix_jacobians"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9eb50dcb-230d-499a-9603-48bfbd0b3944",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "624f2590-032b-4fbc-91a1-66ffe0b2e8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:47:35.475309Z",
     "start_time": "2024-05-17T20:47:35.075774Z"
    }
   },
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "#\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "# fit_kmeans = kmeans.fit(flatten_jacobians)\n",
    "# create_cluster_dirs(data_dir=\"./data\", output_dir=\"clusters/kmeans/jacobians\", labels=fit_kmeans.labels_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "76563b41-d34b-448b-8763-2cda2679fdac",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e39d09d-752c-418b-a5fd-b48701513534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T20:47:35.545473Z",
     "start_time": "2024-05-17T20:47:35.476332Z"
    }
   },
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10, metric=\"precomputed\")\n",
    "fit_db = db.fit(distance_matrix_jacobians)\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(fit_db.labels_)) - (1 if -1 in fit_db.labels_ else 0)\n",
    "n_noise_ = list(fit_db.labels_).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "\n",
    "create_cluster_dirs(data_dir=\"./data\", output_dir=\"clusters/dbscan/jacobians\", labels= fit_db.labels_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6af63ea444008d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:46:33.370672Z",
     "start_time": "2024-05-19T18:46:33.363897Z"
    }
   },
   "source": [
    "img = cv.imread(\"./data/5.33.20.png\", cv.IMREAD_UNCHANGED)\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# define each block as 4x4 cells of 64x64 pixels each\n",
    "cell_size = (5, 5)      # h x w in pixels\n",
    "block_size = (1, 1)         # h x w in cells\n",
    "win_size = (2, 2)           # h x w in cells\n",
    "\n",
    "nbins = 9  # number of orientation bins\n",
    "img_size = img.shape[:2]  # h x w in pixels\n",
    "\n",
    "# create a HOG object\n",
    "hog = cv.HOGDescriptor(\n",
    "    _winSize=(win_size[1] * cell_size[1],\n",
    "              win_size[0] * cell_size[0]),\n",
    "    _blockSize=(block_size[1] * cell_size[1],\n",
    "                block_size[0] * cell_size[0]),\n",
    "    _blockStride=(cell_size[1], cell_size[0]),\n",
    "    _cellSize=(cell_size[1], cell_size[0]),\n",
    "    _nbins=nbins\n",
    ")\n",
    "n_cells = (img_size[0] // cell_size[0], img_size[1] // cell_size[1])\n",
    "\n",
    "# find features as a 1xN vector, then reshape into spatial hierarchy\n",
    "hog_feats_1 = hog.compute(gray)\n",
    "hog_feats_1"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c2b7817cce1705d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:46:34.407579Z",
     "start_time": "2024-05-19T18:46:34.401318Z"
    }
   },
   "source": [
    "img = cv.imread(\"./data/5.33.21.png\", cv.IMREAD_UNCHANGED)\n",
    "gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# define each block as 4x4 cells of 64x64 pixels each\n",
    "cell_size = (5, 5)      # h x w in pixels\n",
    "block_size = (1, 1)         # h x w in cells\n",
    "win_size = (2, 2)           # h x w in cells\n",
    "\n",
    "nbins = 9  # number of orientation bins\n",
    "img_size = img.shape[:2]  # h x w in pixels\n",
    "\n",
    "# create a HOG object\n",
    "hog = cv.HOGDescriptor(\n",
    "    _winSize=(win_size[1] * cell_size[1],\n",
    "              win_size[0] * cell_size[0]),\n",
    "    _blockSize=(block_size[1] * cell_size[1],\n",
    "                block_size[0] * cell_size[0]),\n",
    "    _blockStride=(cell_size[1], cell_size[0]),\n",
    "    _cellSize=(cell_size[1], cell_size[0]),\n",
    "    _nbins=nbins\n",
    ")\n",
    "n_cells = (img_size[0] // cell_size[0], img_size[1] // cell_size[1])\n",
    "\n",
    "# find features as a 1xN vector, then reshape into spatial hierarchy\n",
    "hog_feats = hog.compute(gray)\n",
    "hog_feats"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:46:35.541008Z",
     "start_time": "2024-05-19T18:46:35.536374Z"
    }
   },
   "cell_type": "code",
   "source": "pairwise_distances(hog_feats_1.reshape(1, -1), hog_feats.reshape(1, -1))",
   "id": "ae7a4744e11ad78d",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-19T18:46:45.926863Z",
     "start_time": "2024-05-19T18:46:45.923504Z"
    }
   },
   "cell_type": "code",
   "source": "hog_feats_1.reshape(1, -1)",
   "id": "71b2caf68753d87e",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "e9c35124810c330e",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
