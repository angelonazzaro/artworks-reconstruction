{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27eca293e0492052",
   "metadata": {
    "id": "27eca293e0492052"
   },
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "id": "1lspO5gXGM0z",
   "metadata": {
    "id": "1lspO5gXGM0z"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/content/drive/Shareddrives/artworks-reconstruction')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5123b96350f60dd1",
   "metadata": {
    "id": "5123b96350f60dd1"
   },
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import concurrent.futures\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy import ndimage as ndi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from skimage.feature import hog\n",
    "from skimage.filters import gabor_kernel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from preprocessing.image_conversion import resize_reference_image\n",
    "from clustering import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Creation",
   "id": "c664badd-7f30-4654-914d-b88bca7a11e8"
  },
  {
   "cell_type": "code",
   "id": "8f2d81f2-142e-41f7-bcb6-fecacc84402d",
   "metadata": {
    "id": "8f2d81f2-142e-41f7-bcb6-fecacc84402d"
   },
   "source": [
    "def read_fragments(fragments_dir: str, denoise: bool = True, max_dim: tuple[int, int] = (200, 200)) -> (np.array, dict):\n",
    "    fragments = []\n",
    "    fragments_reading_order = {}\n",
    "\n",
    "    filenames = os.listdir(fragments_dir)\n",
    "    for idx, filename in tqdm(enumerate(filenames), desc=\"Reading fragments\", total=len(filenames)):\n",
    "        fragments_reading_order[filename] = idx\n",
    "        fragment = cv.imread(os.path.join(fragments_dir, filename), cv.IMREAD_UNCHANGED)\n",
    "        fragment = cv.cvtColor(fragment, cv.COLOR_BGR2HSV)\n",
    "        fragment = cv.resize(fragment, max_dim[::-1]) # invert from (w, h) to (h, w) to match numpy's standard\n",
    "\n",
    "        if denoise:\n",
    "            fragment = cv.fastNlMeansDenoisingColored(fragment)\n",
    "\n",
    "        fragments.append(fragment)\n",
    "\n",
    "    return np.array(fragments), fragments_reading_order\n",
    "\n",
    "def read_references(references_dir: str, denoise: bool = True, max_dim: tuple[int, int] = (1000, 1000)) -> (list[np.array], list[int]):\n",
    "    references = []\n",
    "    references_ids = []\n",
    "\n",
    "    filenames = os.listdir(references_dir)\n",
    "    for filename in tqdm(filenames, desc=\"Reading references\", total=len(filenames)):\n",
    "        reference = cv.imread(os.path.join(references_dir, filename), cv.IMREAD_UNCHANGED)\n",
    "        reference = cv.cvtColor(reference, cv.COLOR_BGR2HSV)\n",
    "        reference = resize_reference_image(reference, max_dim[0])\n",
    "\n",
    "        if denoise:\n",
    "            reference = cv.fastNlMeansDenoisingColored(reference)\n",
    "\n",
    "        references.append(reference)\n",
    "        references_ids.append(int(filename.split(\".\")[1]))\n",
    "\n",
    "    return references, references_ids"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##"
   ],
   "metadata": {
    "id": "2l0V4eL4-rd_"
   },
   "id": "2l0V4eL4-rd_"
  },
  {
   "cell_type": "code",
   "id": "f6b434fd-d944-45c9-aba6-01f3daffd182",
   "metadata": {
    "id": "f6b434fd-d944-45c9-aba6-01f3daffd182"
   },
   "source": [
    "def compute_feats(image, kernels):\n",
    "    feats = np.zeros((len(kernels), 2), dtype=np.double)\n",
    "    for k, kernel in enumerate(kernels):\n",
    "        filtered = ndi.convolve(image, kernel, mode='wrap')\n",
    "        feats[k, 0] = filtered.mean()\n",
    "        feats[k, 1] = filtered.var()\n",
    "    return feats\n",
    "\n",
    "def compute_similarity_matrix(feats):\n",
    "    n_images = len(feats)\n",
    "    similarity_matrix = np.zeros((n_images, n_images))\n",
    "    for i in range(n_images):\n",
    "        for j in range(n_images):\n",
    "            similarity_matrix[i, j] = np.linalg.norm(feats[i] - feats[j])\n",
    "    return similarity_matrix\n",
    "\n",
    "# Prepare filter bank kernels\n",
    "kernels = []\n",
    "for theta in range(4):\n",
    "    theta = theta / 4.0 * np.pi\n",
    "    for sigma in (1, 3):\n",
    "        for frequency in (0.05, 0.25):\n",
    "            kernel = np.real(gabor_kernel(frequency, theta=theta, sigma_x=sigma, sigma_y=sigma))\n",
    "            kernels.append(kernel)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0c98ba2-64e2-4d0f-95d5-57c02fc44da3",
   "metadata": {
    "id": "d0c98ba2-64e2-4d0f-95d5-57c02fc44da3"
   },
   "source": [
    "# IN-Cluster Selection\n",
    "\n",
    "The following section describes the algorithm used to select the IN-Cluster. For each candidate cluster $C_i$, for $ i = 1, \\ldots, k $, the following steps are applied:\n",
    "\n",
    "1. Create a composite image from the fragments contained in the cluster $ C_i $, scaling it to the dimensions of the reference image.\n",
    "\n",
    "2. Compute the color similarity by performing a weighted sum of the correlation and intersection of the color histograms between the reference image and the composite image:\n",
    "    \\begin{align}\n",
    "        c\\_sim_{C_i} := -(1 - (c_1|corr| + c_2inter))\n",
    "    \\end{align}\n",
    "where $ |corr| $ is the absolute value of the correlation, and $ c_1 $ and $ c_2 $ are the weights associated with the correlation and intersection of the color histograms, respectively.\n",
    "\n",
    "3. Apply SIFT to identify local feature matches between the reference image and the composite image. From the SIFT results, consider the difference between the number of matches and the median distance to favor clusters with the smallest median distance and the highest number of matches:\n",
    "    \\begin{align}\n",
    "      C_{i_{good}} := n\\_matches - median\\_distance\n",
    "    \\end{align}\n",
    "where $ n\\_matches $ and $ median\\_distance $ are the number of matches and the median distance, respectively.\n",
    "\n",
    "4. Finally, calculate the score of cluster $C_i$ by performing a weighted sum of the previously mentioned measures and the number of fragments in the cluster:\n",
    "    \\begin{align}\n",
    "          score_{C_i} := w_1C_{i_{good}} + w_2 c\\_sim_{C_i} + w_3 n_{C_i}\n",
    "    \\end{align}\n",
    "where $ n_{C_i} $ is the number of fragments in the cluster.\n",
    "\n",
    "The cluster $ C_i $ with the highest $ score_{C_i} $ is selected as the IN-Cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3ccecf1-5148-45ab-8036-4819ce90e8a6",
   "metadata": {
    "id": "c3ccecf1-5148-45ab-8036-4819ce90e8a6"
   },
   "source": [
    "def minmax_norm(values: np.array) -> np.array:\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "\n",
    "    if max_val - min_val == 0:\n",
    "        return 0\n",
    "    return (values - min_val) / (max_val - min_val)\n",
    "\n",
    "\n",
    "def create_composite_image(fragments: np.array, image_size: tuple[int, int, int]) -> np.array:\n",
    "    width, height, channels = image_size\n",
    "    # create a blank canvas for the composite image\n",
    "    composite_image = np.zeros(image_size, dtype=np.uint8)\n",
    "    composite_area = width * height\n",
    "    n_fragments = len(fragments)\n",
    "    fragment_size = int(np.sqrt(composite_area / n_fragments))\n",
    "\n",
    "    x = 0\n",
    "    y = 0\n",
    "    for i in range(n_fragments):\n",
    "        x_offset = x * fragment_size\n",
    "        y_offset = y * fragment_size\n",
    "        size = (fragment_size, fragment_size)\n",
    "\n",
    "        x += 1\n",
    "\n",
    "        area_to_cover = composite_image[x_offset:x_offset + fragment_size, y_offset:y_offset + fragment_size].shape[:2]\n",
    "        # resize the fragment if it goes over outbounds the width or the height\n",
    "        if area_to_cover[0] < fragment_size:\n",
    "            # increment row and reset col where to insert the fragment\n",
    "            x = 0\n",
    "            y += 1\n",
    "            if area_to_cover[0] == 0:\n",
    "                x_offset = x * fragment_size\n",
    "                y_offset = y * fragment_size\n",
    "            else:\n",
    "                size = (area_to_cover[0], fragment_size)\n",
    "        elif area_to_cover[1] < fragment_size:\n",
    "            size = (fragment_size, area_to_cover[1])\n",
    "\n",
    "        # resize the fragment it it goes over outbounds the width or the height\n",
    "\n",
    "        res_fragment = cv.resize(fragments[i], size[::-1])\n",
    "        composite_image[x_offset:x_offset + fragment_size, y_offset:y_offset + fragment_size] = res_fragment\n",
    "\n",
    "    # Handle remaining area if the fragments do not perfectly fill the image\n",
    "    # Find the bounding box around non-black pixels\n",
    "    non_black_pixels = np.argwhere(composite_image.sum(axis=2) > 0)\n",
    "    min_y, min_x = np.min(non_black_pixels, axis=0)\n",
    "    max_y, max_x = np.max(non_black_pixels, axis=0)\n",
    "\n",
    "    # Crop the composite image to the bounding box\n",
    "    cropped_composite_image = composite_image[min_y:max_y+1, min_x:max_x+1]\n",
    "    # Resize the cropped composite image to the original width and height\n",
    "    return cv.resize(cropped_composite_image, (width, height))\n",
    "\n",
    "\n",
    "def compute_SIFT_matches(reference_image: np.array, composite_image: np.array) -> tuple[float, int]:\n",
    "    sift = cv.SIFT_create()\n",
    "    bf = cv.BFMatcher(cv.NORM_L1, crossCheck = False)\n",
    "\n",
    "    composite_image = cv.resize(composite_image, reference_image.shape[:2][::-1])\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Reference Image\")\n",
    "    plt.imshow(reference_image)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Composite Image\")\n",
    "    plt.imshow(composite_image)\n",
    "    plt.show()\n",
    "\n",
    "    composite_image_keypoints, composite_image_descriptor = sift.detectAndCompute(composite_image, None)\n",
    "    reference_keypoints, reference_descriptor = sift.detectAndCompute(reference_image, None)\n",
    "\n",
    "    # Perform the matching between the SIFT descriptors of the fragment image and the reference image\n",
    "    matches = bf.match(composite_image_descriptor, reference_descriptor)\n",
    "    distances = [match.distance for match in matches]\n",
    "    n_distances = len(distances)\n",
    "\n",
    "    if n_distances:\n",
    "        matches_median = np.median(distances)\n",
    "        if not np.isnan(matches_median):\n",
    "            return matches_median, n_distances\n",
    "    return np.nan, n_distances\n",
    "\n",
    "\n",
    "def choose_IN_cluster(reference_image: np.array, reference_id: int, root_dir: str, fragments: np.array, fragments_reading_order: dict,\n",
    "                      image_size: tuple[int, int, int], color_weights: list[int], choice_weights: list[int]) -> (str, str, float, float):\n",
    "\n",
    "    reference_color_histogram = cv.calcHist([reference_image], [0, 1, 2], None, (8, 8, 8), [0, 256, 0, 256, 0, 256])\n",
    "    cv.normalize(reference_color_histogram, reference_color_histogram, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
    "    candidate_clusters = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "    scores = []\n",
    "    recalls = []\n",
    "    distances = []\n",
    "    matches = []\n",
    "    histograms = []\n",
    "    n_fragments = []\n",
    "    total_fragments = len(fragments)\n",
    "\n",
    "    for candidate_cluster in candidate_clusters:\n",
    "        # filter out only the fragments contained in this specific cluster directory\n",
    "        cluster_indexes = [fragments_reading_order[filename] for filename in os.listdir(os.path.join(root_dir, candidate_cluster))]\n",
    "        cluster_fragments = fragments[cluster_indexes]\n",
    "\n",
    "        composite_image = create_composite_image(cluster_fragments, image_size)\n",
    "\n",
    "        composite_color_histogram = cv.calcHist([composite_image], [0, 1, 2], None, (8, 8, 8), [0, 256, 0, 256, 0, 256])\n",
    "        cv.normalize(composite_color_histogram, composite_color_histogram, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
    "        median_distance, n_matches = compute_SIFT_matches(reference_image, composite_image)\n",
    "\n",
    "        distances.append(median_distance)\n",
    "        matches.append(n_matches)\n",
    "        histograms.append(composite_color_histogram)\n",
    "        n_fragments.append(len(cluster_fragments))\n",
    "\n",
    "    max_distance = max(distances)\n",
    "    max_matches = max(matches)\n",
    "\n",
    "    print(f\"Max distance: {max_distance} - Max N. Matches: {max_matches}\")\n",
    "\n",
    "    for i in range(len(candidate_clusters)):\n",
    "        color_correlation = cv.compareHist(reference_color_histogram, histograms[i], cv.HISTCMP_CORREL)\n",
    "        color_intersection = cv.compareHist(reference_color_histogram, histograms[i], cv.HISTCMP_INTERSECT) / np.sum(histograms[i])\n",
    "\n",
    "        color_similarity = (1 - (color_weights[0] * color_correlation + color_weights[1] * color_intersection))\n",
    "\n",
    "        norm_distance = distances[i] / max_distance\n",
    "        norm_matches = matches[i] / max_matches\n",
    "        norm_n_fragments = n_fragments[i] / total_fragments\n",
    "\n",
    "        score = choice_weights[0] * (norm_matches - norm_distance) + choice_weights[1] * color_similarity + choice_weights[2] * norm_n_fragments\n",
    "        recall = recall_in_out_clusters(reference_id, root_dir, [candidate_clusters[i]])\n",
    "        recalls.append(recall)\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f\"Cluster: {candidate_clusters[i]} - Color Similarity: {color_similarity:.2f} - Distance: {distances[i]} - N. Matches: {matches[i]} - Score: {score:.2f} - Recall: {recall:.2f}\")\n",
    "\n",
    "    # Return the cluster directory with the highest score\n",
    "    max_score_idx = np.argmax(scores)\n",
    "    max_recall_idx = np.argmax(recalls)\n",
    "\n",
    "    return candidate_clusters[max_score_idx], candidate_clusters[max_recall_idx], recalls[max_score_idx], recalls[max_recall_idx]\n",
    "\n",
    "\n",
    "def find_elbow_point(values, show_plot = False):\n",
    "  norm_values = (values - values.min()) / (values.max() - values.min())\n",
    "\n",
    "  # Calculate distances to the line formed by the first and last points\n",
    "  n_points = len(norm_values)\n",
    "  all_coords = np.vstack((range(n_points), norm_values)).T\n",
    "  line_vec = all_coords[-1] - all_coords[0]\n",
    "  line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "  vec_from_first = all_coords - all_coords[0]\n",
    "  scalar_product = np.sum(vec_from_first * np.tile(line_vec_norm, (n_points, 1)), axis=1)\n",
    "  vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "  vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "  distances = np.sqrt(np.sum(vec_to_line**2, axis=1))\n",
    "\n",
    "  # Find the elbow point\n",
    "  elbow_idx = np.argmax(distances)\n",
    "  elbow_value = values[elbow_idx]\n",
    "\n",
    "  if show_plot:\n",
    "      plt.figure()\n",
    "      plt.plot(range(n_points), norm_values, 'b-', marker='o')\n",
    "      plt.plot([0, n_points - 1], [norm_values[0], norm_values[-1]], 'r--')\n",
    "      plt.plot(elbow_idx, norm_values[elbow_idx], 'ro')\n",
    "      plt.title('Elbow Point Detection')\n",
    "      plt.xlabel('Index')\n",
    "      plt.ylabel('Values')\n",
    "      plt.show()\n",
    "\n",
    "      print(f\"Elbow idx: {elbow_idx} - Elbow value: {elbow_value}\")\n",
    "\n",
    "  return elbow_idx, elbow_value\n",
    "\n",
    "\n",
    "def recall_in_cluster(reference_image_id: int, reference_cluster_dir: str, data_dir: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the recall of the reference image within the reference cluster directory.\n",
    "\n",
    "    Recall measures the proportion of relevant items that are retrieved.\n",
    "\n",
    "    Args:\n",
    "        reference_image_id (int): The ID of the reference image.\n",
    "        reference_cluster_dir (str): The directory containing the reference cluster images.\n",
    "        data_dir (str): The directory containing the dataset images.\n",
    "\n",
    "    Returns:\n",
    "        float: The recall value.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    tp_filenames = set()\n",
    "\n",
    "    # Count true positives in the reference cluster directory\n",
    "    for filename in os.listdir(reference_cluster_dir):\n",
    "        if filename.split(\".\")[1] == str(reference_image_id):\n",
    "            tp += 1\n",
    "            tp_filenames.add(filename)\n",
    "\n",
    "    # Count false negatives in the data directory\n",
    "    fn = sum(1 for filename in os.listdir(data_dir) if filename.split(\".\")[1] == str(reference_image_id) and filename not in tp_filenames)\n",
    "\n",
    "    total = tp + fn\n",
    "\n",
    "    return tp / total if total else 0.0\n",
    "\n",
    "def remove_empty_bins(histograms):\n",
    "  max_shape = 0\n",
    "  for i in range(len(histograms)):\n",
    "    histograms[i] = histograms[i][np.where(histograms[i] != 0)] # remove empty bins\n",
    "    max_shape = max(max_shape, histograms[i].shape[0])\n",
    "\n",
    "  # Reshape each histogram to have the maximum shape\n",
    "  for i in range(len(histograms)):\n",
    "      current_shape = histograms[i].shape[0]\n",
    "      if current_shape < max_shape:\n",
    "          # Pad with zeros to match the maximum shape\n",
    "          histograms[i] = np.pad(histograms[i], (0, max_shape - current_shape), mode='constant')\n",
    "\n",
    "  return histograms"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d8c85ee-cdc3-454a-a0bf-7d643a9252d3",
   "metadata": {
    "id": "3d8c85ee-cdc3-454a-a0bf-7d643a9252d3"
   },
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "code",
   "id": "856e20c7-faa5-47dc-bd13-a7774c286c75",
   "metadata": {
    "id": "856e20c7-faa5-47dc-bd13-a7774c286c75"
   },
   "source": [
    "dataset_dir = \"datasetQ2\"\n",
    "base_fragments_dir = \"data\"\n",
    "base_references_dir = \"references\"\n",
    "max_fragments_dims = (100, 100)\n",
    "max_references_dims = (1000, 1000)\n",
    "references_working_region_threshold = 20\n",
    "\n",
    "output_dir = \"clusters\"\n",
    "in_clusters_dir = \"in_clusters\"\n",
    "image_size = (1000, 1000, 3)\n",
    "seed = 42\n",
    "weak_pixel_threshold = 50\n",
    "strong_pixel_threshold = 150\n",
    "k = 2\n",
    "\n",
    "choice_weights = [0.5, 0.5, 0.3]\n",
    "color_weights = [0.5, 0.5]\n",
    "\n",
    "patch_similarity_threshold = 0.5\n",
    "\n",
    "max_workers = os.cpu_count()\n",
    "lock = threading.Lock()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Question Datasets\n",
    "\n",
    "In this cell, we create the datasets for a given question (Q1 or Q2). Due to computational limits, we consider only the first 30 rows of each CSV file.\n"
   ],
   "metadata": {
    "id": "ecHOuQkS7bwG"
   },
   "id": "ecHOuQkS7bwG"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the path to the CSV file\n",
    "quesito_dir = '/content/drive/Shareddrives/artworks-reconstruction/Dataset/Quesito_1_Dataset/'\n",
    "top_middle_bottom = ['Top', 'Middle', 'Bottom']\n",
    "left_center_right = ['Sinistra', 'Destra', 'Centrale']\n",
    "\n",
    "quesito_splits = [split_dir for split_dir in os.listdir(quesito_dir) if os.path.isdir(quesito_dir)]\n",
    "quesito_splits.remove('split50_orizzontale')\n",
    "quesito_splits.remove('split50_verticale')\n",
    "\n",
    "for quesito_split in quesito_splits:\n",
    "    quesito_split_dir = os.path.join(quesito_dir, quesito_split)\n",
    "\n",
    "    # Define the output directory for the fragments\n",
    "\n",
    "    for csv_file in os.listdir(quesito_split_dir):\n",
    "        if not csv_file.endswith(\".csv\") or \"referenza\" not in csv_file:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(os.path.join(quesito_split_dir, csv_file))\n",
    "\n",
    "        csv_filebasename = csv_file.split(\".\")[0]\n",
    "\n",
    "        references_dir = f\"{dataset_dir}/{quesito_split}/{csv_filebasename}/{base_references_dir}\"\n",
    "        fragments_dir = f\"{dataset_dir}/{quesito_split}/{csv_filebasename}/{base_fragments_dir}\"\n",
    "\n",
    "        os.makedirs(references_dir, exist_ok=True)\n",
    "        os.makedirs(fragments_dir, exist_ok=True)\n",
    "\n",
    "        row_idx = 0\n",
    "\n",
    "        # Process each row in the CSV file\n",
    "        for index, row in tqdm(df.iterrows(), desc=f\"Creating reference dataset\", total=30):\n",
    "            reference_image_path = row['Reference Image']\n",
    "\n",
    "            if row_idx == 30:\n",
    "                break\n",
    "\n",
    "            row_idx += 1\n",
    "\n",
    "            reference_image_path = reference_image_path.replace(f\"MyDrive{os.path.sep}Dataset_Artworks\",\n",
    "                                                                f\"Shareddrives{os.path.sep}artworks-reconstruction{os.path.sep}\")\n",
    "            reference_new_image_path = os.path.join(references_dir, reference_image_path.split(os.path.sep)[-1])\n",
    "\n",
    "            !cp {reference_image_path} {reference_new_image_path}\n",
    "\n",
    "            for i in range(1, len(df.columns)):\n",
    "                fragment_path = row[f'Associated Fragment Path {i}']\n",
    "\n",
    "                if isinstance(fragment_path, float) and pd.isna(fragment_path):\n",
    "                    break\n",
    "\n",
    "                # replace with actual path\n",
    "                fragment_path = fragment_path.replace(f\"MyDrive{os.path.sep}Dataset_Artworks\",\n",
    "                                                      f\"Shareddrives{os.path.sep}artworks-reconstruction{os.path.sep}\")\n",
    "                fragment_path_split = fragment_path.split(os.path.sep)\n",
    "                fragment_new_name = fragment_path_split[-2] + \"_\" + fragment_path_split[-1]\n",
    "                fragment_new_path = os.path.join(fragments_dir, fragment_new_name)\n",
    "\n",
    "                !cp {fragment_path} {fragment_new_path}"
   ],
   "metadata": {
    "id": "OD6nGsiQ68re"
   },
   "id": "OD6nGsiQ68re",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering\n",
    "\n",
    "In this section, we break down the entire methodology step by step:\n",
    "\n",
    "1. **Dataset Creation**: First, we create a dataset for the clustering phase. This dataset includes the color histograms of the fragments, the color intersection, and texture similarity between individual fragments and the reference image.\n",
    "\n",
    "2. **K-Means Initialization**: To guide the K-Means algorithm, we first apply hierarchical clustering, then compute the centroids and use them as initial points for the K-Means algorithm.\n",
    "\n",
    "3. **IN-Cluster Selection**: We choose the IN-Cluster based on procedure previously described.\n",
    "\n",
    "4. **IN-Cluster Refinement**: To exclude non-pertinent fragments, we refine the IN-Cluster by considering local features between individual fragments and the reference image. We divide the reference image into patches and, for each patch and fragment, compute the color, texture, and edge similarity. A weighted score is calculated, and only fragments whose score surpasses a predefined threshold (set at 0.5) are retained. Due to the computational complexity of the refinement procedure, we parallelize it by assigning different sets of patches and fragments to different cores."
   ],
   "metadata": {
    "id": "D6zvrcrH70DR"
   },
   "id": "D6zvrcrH70DR"
  },
  {
   "cell_type": "code",
   "id": "8835f664-4992-45b3-a0ea-2d925659e672",
   "metadata": {
    "id": "8835f664-4992-45b3-a0ea-2d925659e672"
   },
   "source": [
    "def process_patch(args):\n",
    "    IN_cluster_fragment, IN_fragments_filename, IN_color_histogram, IN_cluster_edge, IN_cluster_texture, i, j, max_fragments_dims, reference_image, kernels = args\n",
    "\n",
    "    patch = reference_image[i:i + max_fragments_dims[0], j:j + max_fragments_dims[1]]\n",
    "    if patch.shape != IN_cluster_fragment.shape:\n",
    "        patch = cv.resize(patch, max_fragments_dims[::-1])\n",
    "\n",
    "    patch_rgb = cv.cvtColor(patch, cv.COLOR_HSV2RGB)\n",
    "    patch_gray = cv.cvtColor(patch_rgb, cv.COLOR_RGB2GRAY)\n",
    "    patch_texture_features = compute_feats(patch_gray, kernels)\n",
    "    patch_texture_features = minmax_norm(patch_texture_features)\n",
    "\n",
    "    if isinstance(patch_texture_features, float) or isinstance(patch_texture_features, int):\n",
    "        return False\n",
    "\n",
    "    patch_color_histogram = cv.calcHist([patch], [0, 1, 2], None, (8, 8, 8), [0, 256, 0, 256, 0, 256])\n",
    "    cv.normalize(patch_color_histogram, patch_color_histogram, alpha=0, beta=1, norm_type=cv.NORM_MINMAX)\n",
    "    patch_color_histogram = patch_color_histogram.flatten()\n",
    "\n",
    "    fragment_color_histogram = IN_color_histogram.flatten()\n",
    "\n",
    "    fragment_patch_color_distance = cv.compareHist(\n",
    "        patch_color_histogram, fragment_color_histogram, cv.HISTCMP_INTERSECT\n",
    "    ) / np.minimum(np.sum(fragment_color_histogram), np.sum(patch_color_histogram))\n",
    "\n",
    "    # Extract edges\n",
    "    patch_edges = cv.Canny(patch_gray, weak_pixel_threshold, strong_pixel_threshold) / 255\n",
    "    edge_sim = cosine_similarity(patch_edges, IN_cluster_edge)[0][0]\n",
    "    texture_sim = cosine_similarity(patch_texture_features, IN_cluster_texture)[0][0]\n",
    "\n",
    "    sim_score = (edge_sim + texture_sim + fragment_patch_color_distance) / 3\n",
    "\n",
    "    if sim_score >= patch_similarity_threshold:\n",
    "        with lock:\n",
    "            refined_IN_fragments.append(IN_cluster_fragment)\n",
    "            refined_filenames.append(IN_fragments_filename)\n",
    "        return True  # Signal to skip the fragment\n",
    "    return False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "G2IEZXQZ6lDo",
   "metadata": {
    "id": "G2IEZXQZ6lDo"
   },
   "source": [
    "split_dirs = [split_dir for split_dir in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, split_dir))]\n",
    "\n",
    "for split_dir in split_dirs:\n",
    "    split_dir_path = os.path.join(dataset_dir, split_dir)\n",
    "    csv_dirs = [csv_dir for csv_dir in os.listdir(split_dir_path) if os.path.isdir(os.path.join(split_dir_path, csv_dir))]\n",
    "    for csv_dir in csv_dirs:\n",
    "        csv_dir_path = os.path.join(split_dir_path, csv_dir)\n",
    "        references_dir = os.path.join(csv_dir_path, base_references_dir)\n",
    "        fragments_dir = os.path.join(csv_dir_path, base_fragments_dir)\n",
    "\n",
    "        fragments, fragments_reading_order = read_fragments(fragments_dir, max_dim=max_fragments_dims)\n",
    "        references, references_ids = read_references(references_dir, max_dim=max_references_dims)\n",
    "        fragments_paths = [os.path.join(fragments_dir, filename) for filename in fragments_reading_order.keys()]\n",
    "\n",
    "        color_histograms = np.array(compute_color_histograms(fragments, flatten=False))\n",
    "        n_fragments = len(fragments)\n",
    "        texture_features = []\n",
    "        hog_features = []\n",
    "        fragment_edges = []\n",
    "\n",
    "        for i in tqdm(range(n_fragments), desc=\"Extracting texture and HOG features\",\n",
    "                      total=n_fragments):\n",
    "            fragment = fragments[i]\n",
    "            fragment = cv.cvtColor(fragment, cv.COLOR_HSV2RGB)\n",
    "            fragment = cv.cvtColor(fragment, cv.COLOR_RGB2GRAY)\n",
    "            fragment_edge = edges = cv.Canny(fragment, weak_pixel_threshold, strong_pixel_threshold)\n",
    "            features, hog_image = hog(fragment_edge, pixels_per_cell=(8, 8),\n",
    "                                      cells_per_block=(2, 2), visualize=True)\n",
    "\n",
    "            fragment_edges.append(fragment_edge)\n",
    "            hog_features.append(features)\n",
    "            texture_features.append(compute_feats(fragment, kernels))\n",
    "\n",
    "        # normalize texture features\n",
    "        texture_features = np.array(texture_features)\n",
    "        normalized_texture_features = minmax_norm(texture_features)\n",
    "\n",
    "        hog_features = np.array(hog_features)\n",
    "        normalized_hog_features = minmax_norm(hog_features)\n",
    "        normalized_hog_features = np.nan_to_num(normalized_hog_features, nan=0.0)\n",
    "\n",
    "        fragment_edges = np.array(fragment_edges)\n",
    "        normalized_fragment_edges = fragment_edges / 255\n",
    "\n",
    "        n_references = len(references_ids)\n",
    "        correct_IN_clusters = 0\n",
    "\n",
    "        metrics_filename = f\"{split_dir}_{csv_dir}.json\"\n",
    "        metrics = {}\n",
    "\n",
    "        for i in range(n_references):\n",
    "            reference_id = references_ids[i]\n",
    "            reference_image = references[i]\n",
    "\n",
    "            print(f\"Iteration {i + 1} out of {n_references} - Reference ID {reference_id}\")\n",
    "            print(\"---------------------------------------------\")\n",
    "\n",
    "            reference_color_histogram = cv.calcHist([reference_image], [0, 1, 2], None, (8, 8, 8),\n",
    "                                                    [0, 256, 0, 256, 0, 256])\n",
    "            cv.normalize(reference_color_histogram, reference_color_histogram, alpha=0, beta=1,\n",
    "                         norm_type=cv.NORM_MINMAX)\n",
    "\n",
    "            # extract region working region and convert to grayscale\n",
    "            reference_image_rgb = cv.cvtColor(reference_image, cv.COLOR_HSV2RGB)\n",
    "            reference_image_gray = cv.cvtColor(reference_image_rgb, cv.COLOR_BGR2GRAY)\n",
    "            # extract reference texture features starting from the borders and normalize them\n",
    "            reference_texture_features = compute_feats(reference_image_gray, kernels)\n",
    "            normalized_reference_texture_features = minmax_norm(reference_texture_features)\n",
    "\n",
    "            X = []\n",
    "            for _ in tqdm(range(n_fragments), desc=f\"Creating dataset for Reference {reference_id}\",\n",
    "                          total=n_fragments):\n",
    "                fragment_reference_color_intersection = cv.compareHist(reference_color_histogram,\n",
    "                                                                       color_histograms[_],\n",
    "                                                                       cv.HISTCMP_INTERSECT) / np.minimum(\n",
    "                    np.sum(color_histograms[_]), np.sum(reference_color_histogram))\n",
    "                X.append(np.hstack((color_histograms[_].reshape(-1), fragment_reference_color_intersection,\n",
    "                                    np.multiply(normalized_texture_features[_],\n",
    "                                                normalized_reference_texture_features).reshape(\n",
    "                                        -1))))  # texture intersection between reference image and fragment\n",
    "            X = np.array(X)\n",
    "\n",
    "            # Use Hierarchical Clustering to find better centroids for K-means init\n",
    "            agg_clust = AgglomerativeClustering(n_clusters=k)\n",
    "            fit_agg_clust = agg_clust.fit(X)\n",
    "            labels = agg_clust.labels_\n",
    "            # Approximate the centroids\n",
    "            centroids = np.array([X[labels == i].mean(axis=0) for i in range(max(labels) + 1)])\n",
    "\n",
    "            kmeans = KMeans(n_clusters=k, random_state=seed, n_init=\"auto\", init=centroids)\n",
    "            fit_kmeans = kmeans.fit(X)\n",
    "\n",
    "            create_cluster_dirs(fragments_paths, output_dir=output_dir, labels=fit_kmeans.labels_)\n",
    "            IN_cluster, correct_IN_cluster, IN_cluster_recall, correct_IN_cluster_recall = choose_IN_cluster(\n",
    "                reference_image,\n",
    "                reference_id,\n",
    "                output_dir,\n",
    "                fragments,\n",
    "                fragments_reading_order,\n",
    "                image_size,\n",
    "                color_weights,\n",
    "                choice_weights)\n",
    "            if IN_cluster == correct_IN_cluster:\n",
    "                correct_IN_clusters += 1\n",
    "\n",
    "            IN_cluster_path = os.path.join(output_dir, IN_cluster)\n",
    "\n",
    "            IN_cluster_fragments_names = [filename for filename in os.listdir(IN_cluster_path)]\n",
    "            IN_cluster_indexes = [fragments_reading_order[filename] for filename in IN_cluster_fragments_names]\n",
    "            IN_cluster_fragments = fragments[IN_cluster_indexes]\n",
    "            IN_color_histograms = color_histograms[IN_cluster_indexes]\n",
    "\n",
    "            print(f\"Recall: {recall_in_out_clusters(reference_id, output_dir, [IN_cluster]):.2f}\")\n",
    "\n",
    "            IN_cluster_fragments_flatten = IN_cluster_fragments.reshape(IN_cluster_fragments.shape[0], -1)\n",
    "\n",
    "            # refine IN Cluster\n",
    "            n_IN_fragments = len(IN_cluster_fragments)\n",
    "            IN_cluster_edges = normalized_fragment_edges[IN_cluster_indexes]\n",
    "            IN_cluster_textures = normalized_texture_features[IN_cluster_indexes]\n",
    "            IN_color_histograms = color_histograms[IN_cluster_indexes]\n",
    "            IN_fragments_filenames = [filename for filename, idx in fragments_reading_order.items() if\n",
    "                                      idx in IN_cluster_indexes]\n",
    "\n",
    "            refined_IN_fragments = []\n",
    "            refined_filenames = []\n",
    "            skip_fragment = False\n",
    "\n",
    "            for _ in tqdm(range(n_IN_fragments), desc=f\"Extracting patches from Reference ID {reference_id}\"):\n",
    "                tasks = []\n",
    "                for i in range(0, reference_image.shape[0], max_fragments_dims[0]):\n",
    "                    for j in range(0, reference_image.shape[1], max_fragments_dims[1]):\n",
    "                        tasks.append((IN_cluster_fragments[_], IN_fragments_filenames[_], IN_color_histograms[_], IN_cluster_edges[_], IN_cluster_textures[_], i, j, max_fragments_dims, reference_image, kernels))\n",
    "\n",
    "                # Using ThreadPoolExecutor to process patches in parallel\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    results = list(executor.map(process_patch, tasks))\n",
    "\n",
    "                    # Wait for all threads to finish\n",
    "                    executor.shutdown(wait=True)\n",
    "\n",
    "            refined_IN_fragments_flatten = np.array(refined_IN_fragments)\n",
    "            refined_IN_fragments_flatten = refined_IN_fragments_flatten.reshape(\n",
    "                refined_IN_fragments_flatten.shape[0], -1)\n",
    "\n",
    "            # copy fragments and create refined IN-cluster dir\n",
    "            refined_IN_cluster_path = os.path.join(in_clusters_dir, str(reference_id))\n",
    "\n",
    "            if os.path.exists(refined_IN_cluster_path):\n",
    "                shutil.rmtree(refined_IN_cluster_path)\n",
    "            os.makedirs(refined_IN_cluster_path, exist_ok=True)\n",
    "\n",
    "            for filename in tqdm(refined_filenames,\n",
    "                                 desc=f\"Copying fragments to refined IN-Cluster dir for Reference ID {reference_id}\",\n",
    "                                 total=len(refined_filenames)):\n",
    "                shutil.copy(os.path.join(IN_cluster_path, filename),\n",
    "                            os.path.join(refined_IN_cluster_path, filename))\n",
    "\n",
    "            # compute pre-refining metrics\n",
    "            metrics[reference_id] = {\n",
    "                \"pre_refinement_precision\": precision(reference_id, IN_cluster_path),\n",
    "                \"pre_refinement_recall\": recall_in_out_clusters(reference_id, output_dir, [IN_cluster]),\n",
    "                \"pre_refinement_number_of_fragments\": len(IN_cluster_fragments),\n",
    "                \"post_refinement_precision\": precision(reference_id, refined_IN_cluster_path),\n",
    "                \"post_refinement_recall\": recall_in_cluster(reference_id, refined_IN_cluster_path, fragments_dir),\n",
    "                \"post_refinement_number_of_fragments\": len(refined_filenames),\n",
    "                \"reduction\": 1 - (len(refined_filenames) / len(IN_cluster_fragments))\n",
    "            }\n",
    "\n",
    "        correct_guesses = correct_IN_clusters / n_references\n",
    "\n",
    "        print(\"\\n\\nTerminated!\\n\\n\")\n",
    "        print(\n",
    "            f\"Correct IN cluster selected: {correct_IN_clusters}/{n_references} --- {(correct_guesses) * 100:.2f}%\")\n",
    "\n",
    "        metrics[\"barbara\"] = {\n",
    "            \"correct_choices\": f\"{correct_IN_clusters}/{n_references}\",\n",
    "            \"correct_choices_perc\": f\"{(correct_guesses) * 100:.2f}\"\n",
    "        }\n",
    "\n",
    "        with open(metrics_filename, \"w\") as f:\n",
    "            json.dump(metrics, f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fragments, fragments_reading_order = read_fragments(base_fragments_dir, max_dim=max_fragments_dims)\n",
    "references, references_ids = read_references(base_references_dir, max_dim=max_references_dims)\n",
    "fragments_paths = [os.path.join(base_fragments_dir, filename) for filename in fragments_reading_order.keys()]\n",
    "\n",
    "color_histograms = np.array(compute_color_histograms(fragments, flatten=False))\n",
    "\n",
    "n_fragments = len(fragments)\n",
    "texture_features = []\n",
    "hog_features = []\n",
    "fragment_edges = []\n",
    "\n",
    "for i in tqdm(range(n_fragments), desc=\"Extracting texture and HOG features\",\n",
    "              total=n_fragments):\n",
    "    fragment = fragments[i]\n",
    "    fragment = cv.cvtColor(fragment, cv.COLOR_HSV2RGB)\n",
    "    fragment = cv.cvtColor(fragment, cv.COLOR_RGB2GRAY)\n",
    "    fragment_edge = edges = cv.Canny(fragment, weak_pixel_threshold, strong_pixel_threshold)\n",
    "    features, hog_image = hog(fragment_edge, pixels_per_cell=(8, 8),\n",
    "                              cells_per_block=(2, 2), visualize=True)\n",
    "\n",
    "    fragment_edges.append(fragment_edge)\n",
    "    hog_features.append(features)\n",
    "    texture_features.append(compute_feats(fragment, kernels))\n",
    "\n",
    "# normalize texture features\n",
    "texture_features = np.array(texture_features)\n",
    "normalized_texture_features = minmax_norm(texture_features)\n",
    "\n",
    "hog_features = np.array(hog_features)\n",
    "normalized_hog_features = minmax_norm(hog_features)\n",
    "normalized_hog_features = np.nan_to_num(normalized_hog_features, nan=0.0)\n",
    "\n",
    "fragment_edges = np.array(fragment_edges)\n",
    "normalized_fragment_edges = fragment_edges / 255\n",
    "\n",
    "n_references = len(references_ids)\n",
    "correct_IN_clusters = 0\n",
    "\n",
    "metrics_filename = f\"metrics.json\"\n",
    "metrics = {}\n",
    "\n",
    "for i in range(n_references):\n",
    "    reference_id = references_ids[i]\n",
    "    reference_image = references[i]\n",
    "\n",
    "    print(f\"Iteration {i + 1} out of {n_references} - Reference ID {reference_id}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "    reference_color_histogram = cv.calcHist([reference_image], [0, 1, 2], None, (8, 8, 8),\n",
    "                                            [0, 256, 0, 256, 0, 256])\n",
    "    cv.normalize(reference_color_histogram, reference_color_histogram, alpha=0, beta=1,\n",
    "                 norm_type=cv.NORM_MINMAX)\n",
    "\n",
    "    # extract region working region and convert to grayscale\n",
    "    reference_image_rgb = cv.cvtColor(reference_image, cv.COLOR_HSV2RGB)\n",
    "    reference_image_gray = cv.cvtColor(reference_image_rgb, cv.COLOR_BGR2GRAY)\n",
    "    # extract reference texture features starting from the borders and normalize them\n",
    "    reference_texture_features = compute_feats(reference_image_gray, kernels)\n",
    "    normalized_reference_texture_features = minmax_norm(reference_texture_features)\n",
    "\n",
    "    X = []\n",
    "    for _ in tqdm(range(n_fragments), desc=f\"Creating dataset for Reference {reference_id}\",\n",
    "                  total=n_fragments):\n",
    "        fragment_reference_color_intersection = cv.compareHist(reference_color_histogram,\n",
    "                                                               color_histograms[_],\n",
    "                                                               cv.HISTCMP_INTERSECT) / np.minimum(\n",
    "            np.sum(color_histograms[_]), np.sum(reference_color_histogram))\n",
    "        X.append(np.hstack((color_histograms[_].reshape(-1), fragment_reference_color_intersection,\n",
    "                            np.multiply(normalized_texture_features[_],\n",
    "                                        normalized_reference_texture_features).reshape(\n",
    "                                -1))))  # texture intersection between reference image and fragment\n",
    "    X = np.array(X)\n",
    "\n",
    "    # Use Hierarchical Clustering to find better centroids for K-means init\n",
    "    agg_clust = AgglomerativeClustering(n_clusters=k)\n",
    "    fit_agg_clust = agg_clust.fit(X)\n",
    "    labels = agg_clust.labels_\n",
    "    # Approximate the centroids\n",
    "    centroids = np.array([X[labels == i].mean(axis=0) for i in range(max(labels) + 1)])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k, random_state=seed, n_init=\"auto\", init=centroids)\n",
    "    fit_kmeans = kmeans.fit(X)\n",
    "\n",
    "    create_cluster_dirs(fragments_paths, output_dir=output_dir, labels=fit_kmeans.labels_)\n",
    "    IN_cluster, correct_IN_cluster, IN_cluster_recall, correct_IN_cluster_recall = choose_IN_cluster(\n",
    "        reference_image,\n",
    "        reference_id,\n",
    "        output_dir,\n",
    "        fragments,\n",
    "        fragments_reading_order,\n",
    "        image_size,\n",
    "        color_weights,\n",
    "        choice_weights)\n",
    "    if IN_cluster == correct_IN_cluster:\n",
    "        correct_IN_clusters += 1\n",
    "\n",
    "    IN_cluster_path = os.path.join(output_dir, IN_cluster)\n",
    "\n",
    "    IN_cluster_fragments_names = [filename for filename in os.listdir(IN_cluster_path)]\n",
    "    IN_cluster_indexes = [fragments_reading_order[filename] for filename in IN_cluster_fragments_names]\n",
    "    IN_cluster_fragments = fragments[IN_cluster_indexes]\n",
    "    IN_color_histograms = color_histograms[IN_cluster_indexes]\n",
    "\n",
    "    print(f\"Recall: {recall_in_out_clusters(reference_id, output_dir, [IN_cluster]):.2f}\")\n",
    "\n",
    "    IN_cluster_fragments_flatten = IN_cluster_fragments.reshape(IN_cluster_fragments.shape[0], -1)\n",
    "\n",
    "    # refine IN Cluster\n",
    "    n_IN_fragments = len(IN_cluster_fragments)\n",
    "    IN_cluster_edges = normalized_fragment_edges[IN_cluster_indexes]\n",
    "    IN_cluster_textures = normalized_texture_features[IN_cluster_indexes]\n",
    "    IN_color_histograms = color_histograms[IN_cluster_indexes]\n",
    "    IN_fragments_filenames = [filename for filename, idx in fragments_reading_order.items() if\n",
    "                              idx in IN_cluster_indexes]\n",
    "\n",
    "    refined_IN_fragments = []\n",
    "    refined_filenames = []\n",
    "    skip_fragment = False\n",
    "\n",
    "    for _ in tqdm(range(n_IN_fragments), desc=f\"Extracting patches from Reference ID {reference_id}\"):\n",
    "        tasks = []\n",
    "        for i in range(0, reference_image.shape[0], max_fragments_dims[0]):\n",
    "            for j in range(0, reference_image.shape[1], max_fragments_dims[1]):\n",
    "                tasks.append((IN_cluster_fragments[_], IN_fragments_filenames[_], IN_color_histograms[_], IN_cluster_edges[_], IN_cluster_textures[_], i, j, max_fragments_dims, reference_image, kernels))\n",
    "\n",
    "        # Using ThreadPoolExecutor to process patches in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(executor.map(process_patch, tasks))\n",
    "\n",
    "            # Wait for all threads to finish\n",
    "            executor.shutdown(wait=True)\n",
    "\n",
    "    refined_IN_fragments_flatten = np.array(refined_IN_fragments)\n",
    "    refined_IN_fragments_flatten = refined_IN_fragments_flatten.reshape(\n",
    "        refined_IN_fragments_flatten.shape[0], -1)\n",
    "\n",
    "    # copy fragments and create refined IN-cluster dir\n",
    "    refined_IN_cluster_path = os.path.join(in_clusters_dir, str(reference_id))\n",
    "\n",
    "    if os.path.exists(refined_IN_cluster_path):\n",
    "        shutil.rmtree(refined_IN_cluster_path)\n",
    "    os.makedirs(refined_IN_cluster_path, exist_ok=True)\n",
    "\n",
    "    for filename in tqdm(refined_filenames,\n",
    "                         desc=f\"Copying fragments to refined IN-Cluster dir for Reference ID {reference_id}\",\n",
    "                         total=len(refined_filenames)):\n",
    "        shutil.copy(os.path.join(IN_cluster_path, filename),\n",
    "                    os.path.join(refined_IN_cluster_path, filename))\n",
    "\n",
    "    # compute pre-refining metrics\n",
    "    metrics[reference_id] = {\n",
    "        \"pre_refinement_precision\": precision(reference_id, IN_cluster_path),\n",
    "        \"pre_refinement_recall\": recall_in_out_clusters(reference_id, output_dir, [IN_cluster]),\n",
    "        \"pre_refinement_number_of_fragments\": len(IN_cluster_fragments),\n",
    "        \"post_refinement_precision\": precision(reference_id, refined_IN_cluster_path),\n",
    "        \"post_refinement_recall\": recall_in_cluster(reference_id, refined_IN_cluster_path, base_fragments_dir),\n",
    "        \"post_refinement_number_of_fragments\": len(refined_filenames),\n",
    "        \"reduction\": 1 - (len(refined_filenames) / len(IN_cluster_fragments))\n",
    "    }\n",
    "\n",
    "correct_guesses = correct_IN_clusters / n_references\n",
    "\n",
    "print(\"\\n\\nTerminated!\\n\\n\")\n",
    "print(\n",
    "    f\"Correct IN cluster selected: {correct_IN_clusters}/{n_references} --- {(correct_guesses) * 100:.2f}%\")\n",
    "\n",
    "metrics[\"barbara\"] = {\n",
    "    \"correct_choices\": f\"{correct_IN_clusters}/{n_references}\",\n",
    "    \"correct_choices_perc\": f\"{(correct_guesses) * 100:.2f}\"\n",
    "}\n",
    "\n",
    "with open(metrics_filename, \"w\") as f:\n",
    "    json.dump(metrics, f)"
   ],
   "id": "6dd3cd7646c2da2d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
